{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Foundation\n",
    "\n",
    "## Course 2, Part b: Regression Setup, Train-test Split LAB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We will be working with a data set based on [housing prices in Ames, Iowa](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). It was compiled for educational use to be a modernized and expanded alternative to the well-known Boston Housing dataset. This version of the data set has had some missing values filled for convenience.\n",
    "\n",
    "There are an extensive number of features, so they've been described in the table below.\n",
    "\n",
    "### Predictor\n",
    "\n",
    "* SalePrice: The property's sale price in dollars. \n",
    "\n",
    "### Features\n",
    "\n",
    "* MoSold: Month Sold\n",
    "* YrSold: Year Sold   \n",
    "* SaleType: Type of sale\n",
    "* SaleCondition: Condition of sale\n",
    "* MSSubClass: The building class\n",
    "* MSZoning: The general zoning classification\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-03-09T17:24:40.724060Z",
     "start_time": "2017-03-09T12:24:40.718739-05:00"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = ['C:\\\\Users\\\\AlexP\\\\Documents\\\\Project\\\\IBM_ML_Course\\\\Course_2\\\\Week_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 1\n",
    "\n",
    "* Import the data using Pandas and examine the shape. There are 79 feature columns plus the predictor, the sale price (`SalePrice`). \n",
    "* There are three different types: integers (`int64`), floats (`float64`), and strings (`object`, categoricals). Examine how many there are of each data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object     43\n",
      "float64    21\n",
      "int64      16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = os.sep.join(data_path + ['Ames_Housing_Sales.csv'])\n",
    "data = pd.read_csv(filepath, sep=',')\n",
    "\n",
    "print(data.dtypes.value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "object     43\nfloat64    21\nint64      16\ndtype: int64"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "A significant challenge, particularly when dealing with data that have many columns, is ensuring each column gets encoded correctly. \n",
    "\n",
    "This is particularly true with data columns that are ordered categoricals (ordinals) vs unordered categoricals. Unordered categoricals should be one-hot encoded, however this can significantly increase the number of features and creates features that are highly correlated with each other.\n",
    "\n",
    "Determine how many total features would be present, relative to what currently exists, if all string (object) features are one-hot encoded. Recall that the total number of one-hot encoded columns is `n-1`, where `n` is the number of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "uint8      258\nfloat64     21\nint64       16\ndtype: int64"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "data_dummies = pd.get_dummies(data)\n",
    "data_dummies.dtypes.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\alexp\\documents\\project\\ibm_ml_course\\venv\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mask_data = data.dtypes ==np.object\n",
    "categorical_cols = data.columns[mask_data]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Alley             3\nBldgType          5\nBsmtCond          4\nBsmtExposure      5\nBsmtFinType1      6\nBsmtFinType2      7\nBsmtQual          5\nCentralAir        2\nCondition1        9\nCondition2        8\nElectrical        5\nExterCond         4\nExterQual         4\nExterior1st      14\nExterior2nd      16\nFence             5\nFireplaceQu       6\nFoundation        6\nFunctional        7\nGarageCond        5\nGarageFinish      3\nGarageQual        5\nGarageType        6\nHeating           6\nHeatingQC         5\nHouseStyle        8\nKitchenQual       4\nLandContour       4\nLandSlope         3\nLotConfig         5\nLotShape          4\nMSZoning          5\nMasVnrType        4\nMiscFeature       5\nNeighborhood     25\nPavedDrive        3\nPoolQC            4\nRoofMatl          8\nRoofStyle         6\nSaleCondition     6\nSaleType          9\nStreet            2\nUtilities         2\ndtype: int64"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_ohc_cols = (data[categorical_cols]\n",
    "                .apply(lambda x: len(x.unique())))\n",
    "num_ohc_cols"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "215"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No need to encode if there is only one value\n",
    "small_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n",
    "small_num_ohc_cols -= 1\n",
    "small_num_ohc_cols.sum()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (2,) ",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-9-5ea8e7766d79>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      2\u001B[0m num_ohc_cols = (data[categorical_cols]\n\u001B[0;32m      3\u001B[0m                 \u001B[1;33m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;32mlambda\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m                 .sort_values(ascending=False))\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\alexp\\documents\\project\\ibm_ml_course\\venv\\lib\\site-packages\\pandas\\core\\series.py\u001B[0m in \u001B[0;36msort_values\u001B[1;34m(self, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001B[0m\n\u001B[0;32m   3261\u001B[0m         \u001B[1;31m# GH 35922. Make sorting stable by leveraging nargsort\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3262\u001B[0m         \u001B[0mvalues_to_sort\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mensure_key_mapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3263\u001B[1;33m         \u001B[0msorted_index\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnargsort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalues_to_sort\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkind\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbool\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mascending\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mna_position\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3264\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3265\u001B[0m         result = self._constructor(\n",
      "\u001B[1;32mc:\\users\\alexp\\documents\\project\\ibm_ml_course\\venv\\lib\\site-packages\\pandas\\core\\sorting.py\u001B[0m in \u001B[0;36mnargsort\u001B[1;34m(items, kind, ascending, na_position, key, mask)\u001B[0m\n\u001B[0;32m    378\u001B[0m         \u001B[0mnon_nans\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnon_nans\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m         \u001B[0mnon_nan_idx\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnon_nan_idx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 380\u001B[1;33m     \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnon_nan_idx\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mnon_nans\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkind\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mkind\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    381\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mascending\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[0mindexer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mindexer\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: operands could not be broadcast together with shapes (5,) (2,) "
     ]
    }
   ],
   "source": [
    "# Determine how many extra columns would be created\n",
    "num_ohc_cols = (data[categorical_cols]\n",
    "                .apply(lambda x: x.unique())\n",
    "                .sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# No need to encode if there is only one value\n",
    "small_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n",
    "\n",
    "# Number of one-hot columns is one less than the number of categories\n",
    "small_num_ohc_cols -= 1\n",
    "\n",
    "# This is 215 columns, assuming the original ones are dropped. \n",
    "# This is quite a few extra columns!\n",
    "small_num_ohc_cols.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "Let's create a new data set where all of the above categorical features will be one-hot encoded. We can fit this data and see how it affects the results.\n",
    "\n",
    "* Used the dataframe `.copy()` method to create a completely separate copy of the dataframe for one-hot encoding\n",
    "* On this new dataframe, one-hot encode each of the appropriate columns and add it back to the dataframe. Be sure to drop the original column.\n",
    "* For the data that are not one-hot encoded, drop the columns that are string categoricals.\n",
    "\n",
    "For the first step, numerically encoding the string categoricals, either Scikit-learn;s `LabelEncoder` or `DictVectorizer` can be used. However, the former is probably easier since it doesn't require specifying a numerical value for each category, and we are going to one-hot encode all of the numerical values anyway. (Can you think of a time when `DictVectorizer` might be preferred?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(1379, 6)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "data_copy = data.copy(deep=True)\n",
    "two_test_feature = data_copy.loc[:, [\"BsmtFinType1\"]]\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "a = encoder.fit_transform(X=two_test_feature)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "(8274, 1)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.reshape(-1, 1)\n",
    "b.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "     Alley BldgType BsmtCond BsmtExposure BsmtFinType1 BsmtFinType2 BsmtQual  \\\n0     None     1Fam       TA           No          GLQ          Unf       Gd   \n1     None     1Fam       TA           Gd          ALQ          Unf       Gd   \n2     None     1Fam       TA           Mn          GLQ          Unf       Gd   \n3     None     1Fam       Gd           No          ALQ          Unf       TA   \n4     None     1Fam       TA           Av          GLQ          Unf       Gd   \n...    ...      ...      ...          ...          ...          ...      ...   \n1374  None     1Fam     None         None         None         None     None   \n1375  None     1Fam       TA           No          ALQ          Rec       Gd   \n1376  None     1Fam       Gd           No          GLQ          Unf       TA   \n1377  None     1Fam       TA           Mn          GLQ          Rec       TA   \n1378  None     1Fam       TA           No          BLQ          LwQ       TA   \n\n     CentralAir Condition1 Condition2  ... MiscFeature Neighborhood  \\\n0             Y       Norm       Norm  ...        None      CollgCr   \n1             Y      Feedr       Norm  ...        None      Veenker   \n2             Y       Norm       Norm  ...        None      CollgCr   \n3             Y       Norm       Norm  ...        None      Crawfor   \n4             Y       Norm       Norm  ...        None      NoRidge   \n...         ...        ...        ...  ...         ...          ...   \n1374          Y       Norm       Norm  ...        None      Gilbert   \n1375          Y       Norm       Norm  ...        None       NWAmes   \n1376          Y       Norm       Norm  ...        Shed      Crawfor   \n1377          Y       Norm       Norm  ...        None        NAmes   \n1378          Y       Norm       Norm  ...        None      Edwards   \n\n     PavedDrive PoolQC RoofMatl RoofStyle SaleCondition SaleType Street  \\\n0             Y   None  CompShg     Gable        Normal       WD   Pave   \n1             Y   None  CompShg     Gable        Normal       WD   Pave   \n2             Y   None  CompShg     Gable        Normal       WD   Pave   \n3             Y   None  CompShg     Gable       Abnorml       WD   Pave   \n4             Y   None  CompShg     Gable        Normal       WD   Pave   \n...         ...    ...      ...       ...           ...      ...    ...   \n1374          Y   None  CompShg     Gable        Normal       WD   Pave   \n1375          Y   None  CompShg     Gable        Normal       WD   Pave   \n1376          Y   None  CompShg     Gable        Normal       WD   Pave   \n1377          Y   None  CompShg       Hip        Normal       WD   Pave   \n1378          Y   None  CompShg     Gable        Normal       WD   Pave   \n\n     Utilities  \n0       AllPub  \n1       AllPub  \n2       AllPub  \n3       AllPub  \n4       AllPub  \n...        ...  \n1374    AllPub  \n1375    AllPub  \n1376    AllPub  \n1377    AllPub  \n1378    AllPub  \n\n[1379 rows x 43 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Alley</th>\n      <th>BldgType</th>\n      <th>BsmtCond</th>\n      <th>BsmtExposure</th>\n      <th>BsmtFinType1</th>\n      <th>BsmtFinType2</th>\n      <th>BsmtQual</th>\n      <th>CentralAir</th>\n      <th>Condition1</th>\n      <th>Condition2</th>\n      <th>...</th>\n      <th>MiscFeature</th>\n      <th>Neighborhood</th>\n      <th>PavedDrive</th>\n      <th>PoolQC</th>\n      <th>RoofMatl</th>\n      <th>RoofStyle</th>\n      <th>SaleCondition</th>\n      <th>SaleType</th>\n      <th>Street</th>\n      <th>Utilities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>GLQ</td>\n      <td>Unf</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>CollgCr</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Gd</td>\n      <td>ALQ</td>\n      <td>Unf</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>Feedr</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Veenker</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Mn</td>\n      <td>GLQ</td>\n      <td>Unf</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>CollgCr</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>Gd</td>\n      <td>No</td>\n      <td>ALQ</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Crawfor</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Abnorml</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Av</td>\n      <td>GLQ</td>\n      <td>Unf</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>NoRidge</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1374</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>None</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Gilbert</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>1375</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>ALQ</td>\n      <td>Rec</td>\n      <td>Gd</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>NWAmes</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>1376</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>Gd</td>\n      <td>No</td>\n      <td>GLQ</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>Shed</td>\n      <td>Crawfor</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>1377</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>Mn</td>\n      <td>GLQ</td>\n      <td>Rec</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>NAmes</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Hip</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n    <tr>\n      <th>1378</th>\n      <td>None</td>\n      <td>1Fam</td>\n      <td>TA</td>\n      <td>No</td>\n      <td>BLQ</td>\n      <td>LwQ</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Norm</td>\n      <td>Norm</td>\n      <td>...</td>\n      <td>None</td>\n      <td>Edwards</td>\n      <td>Y</td>\n      <td>None</td>\n      <td>CompShg</td>\n      <td>Gable</td>\n      <td>Normal</td>\n      <td>WD</td>\n      <td>Pave</td>\n      <td>AllPub</td>\n    </tr>\n  </tbody>\n</table>\n<p>1379 rows × 43 columns</p>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_copy[data_copy.columns[data_copy.dtypes == np.object_]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "      1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n0        856.0     854.0        0.0             3       706.0         0.0   \n1       1262.0       0.0        0.0             3       978.0         0.0   \n2        920.0     866.0        0.0             3       486.0         0.0   \n3        961.0     756.0        0.0             3       216.0         0.0   \n4       1145.0    1053.0        0.0             4       655.0         0.0   \n...        ...       ...        ...           ...         ...         ...   \n1374     953.0     694.0        0.0             3         0.0         0.0   \n1375    2073.0       0.0        0.0             3       790.0       163.0   \n1376    1188.0    1152.0        0.0             4       275.0         0.0   \n1377    1078.0       0.0        0.0             2        49.0      1029.0   \n1378    1256.0       0.0        0.0             3       830.0       290.0   \n\n      BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  OverallQual  \\\n0                1             0      150.0            0.0  ...            7   \n1                0             1      284.0            0.0  ...            6   \n2                1             0      434.0            0.0  ...            7   \n3                1             0      540.0          272.0  ...            7   \n4                1             0      490.0            0.0  ...            8   \n...            ...           ...        ...            ...  ...          ...   \n1374             0             0      953.0            0.0  ...            6   \n1375             1             0      589.0            0.0  ...            6   \n1376             0             0      877.0            0.0  ...            7   \n1377             1             0        0.0          112.0  ...            5   \n1378             1             0      136.0            0.0  ...            5   \n\n      PoolArea  ScreenPorch  TotRmsAbvGrd  TotalBsmtSF  WoodDeckSF  YearBuilt  \\\n0          0.0          0.0             8        856.0         0.0       2003   \n1          0.0          0.0             6       1262.0       298.0       1976   \n2          0.0          0.0             6        920.0         0.0       2001   \n3          0.0          0.0             7        756.0         0.0       1915   \n4          0.0          0.0             9       1145.0       192.0       2000   \n...        ...          ...           ...          ...         ...        ...   \n1374       0.0          0.0             7        953.0         0.0       1999   \n1375       0.0          0.0             7       1542.0       349.0       1978   \n1376       0.0          0.0             9       1152.0         0.0       1941   \n1377       0.0          0.0             5       1078.0       366.0       1950   \n1378       0.0          0.0             6       1256.0       736.0       1965   \n\n      YearRemodAdd  YrSold  SalePrice  \n0             2003    2008   208500.0  \n1             1976    2007   181500.0  \n2             2002    2008   223500.0  \n3             1970    2006   140000.0  \n4             2000    2008   250000.0  \n...            ...     ...        ...  \n1374          2000    2007   175000.0  \n1375          1988    2010   210000.0  \n1376          2006    2010   266500.0  \n1377          1996    2010   142125.0  \n1378          1965    2008   147500.0  \n\n[1379 rows x 37 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>3SsnPorch</th>\n      <th>BedroomAbvGr</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>BsmtFullBath</th>\n      <th>BsmtHalfBath</th>\n      <th>BsmtUnfSF</th>\n      <th>EnclosedPorch</th>\n      <th>...</th>\n      <th>OverallQual</th>\n      <th>PoolArea</th>\n      <th>ScreenPorch</th>\n      <th>TotRmsAbvGrd</th>\n      <th>TotalBsmtSF</th>\n      <th>WoodDeckSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>YrSold</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>856.0</td>\n      <td>854.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>706.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>150.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>856.0</td>\n      <td>0.0</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>2008</td>\n      <td>208500.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1262.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>978.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>284.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1262.0</td>\n      <td>298.0</td>\n      <td>1976</td>\n      <td>1976</td>\n      <td>2007</td>\n      <td>181500.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>920.0</td>\n      <td>866.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>486.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>434.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>920.0</td>\n      <td>0.0</td>\n      <td>2001</td>\n      <td>2002</td>\n      <td>2008</td>\n      <td>223500.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>961.0</td>\n      <td>756.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>216.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>540.0</td>\n      <td>272.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>756.0</td>\n      <td>0.0</td>\n      <td>1915</td>\n      <td>1970</td>\n      <td>2006</td>\n      <td>140000.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1145.0</td>\n      <td>1053.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>655.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>490.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>1145.0</td>\n      <td>192.0</td>\n      <td>2000</td>\n      <td>2000</td>\n      <td>2008</td>\n      <td>250000.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1374</th>\n      <td>953.0</td>\n      <td>694.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>953.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>953.0</td>\n      <td>0.0</td>\n      <td>1999</td>\n      <td>2000</td>\n      <td>2007</td>\n      <td>175000.0</td>\n    </tr>\n    <tr>\n      <th>1375</th>\n      <td>2073.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>790.0</td>\n      <td>163.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>589.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>1542.0</td>\n      <td>349.0</td>\n      <td>1978</td>\n      <td>1988</td>\n      <td>2010</td>\n      <td>210000.0</td>\n    </tr>\n    <tr>\n      <th>1376</th>\n      <td>1188.0</td>\n      <td>1152.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>275.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>877.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9</td>\n      <td>1152.0</td>\n      <td>0.0</td>\n      <td>1941</td>\n      <td>2006</td>\n      <td>2010</td>\n      <td>266500.0</td>\n    </tr>\n    <tr>\n      <th>1377</th>\n      <td>1078.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>49.0</td>\n      <td>1029.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>112.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>1078.0</td>\n      <td>366.0</td>\n      <td>1950</td>\n      <td>1996</td>\n      <td>2010</td>\n      <td>142125.0</td>\n    </tr>\n    <tr>\n      <th>1378</th>\n      <td>1256.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>830.0</td>\n      <td>290.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>136.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1256.0</td>\n      <td>736.0</td>\n      <td>1965</td>\n      <td>1965</td>\n      <td>2008</td>\n      <td>147500.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1379 rows × 37 columns</p>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.columns\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Copy of the data\n",
    "data_ohc = data.copy()\n",
    "\n",
    "# The encoders\n",
    "le = LabelEncoder()\n",
    "ohc = OneHotEncoder()\n",
    "\n",
    "for col in num_ohc_cols.index:\n",
    "    \n",
    "    # Integer encode the string categories\n",
    "    dat = le.fit_transform(data_ohc[col]).astype(np.int_)\n",
    "    # Remove the original column from the dataframe\n",
    "    data_ohc = data_ohc.drop(col, axis=1)\n",
    "\n",
    "    # One hot encode the data--this returns a sparse array\n",
    "    new_dat = ohc.fit_transform(dat.reshape(-1,1))\n",
    "\n",
    "    # Create unique column names\n",
    "    n_cols = new_dat.shape[1]\n",
    "    col_names = ['_'.join([col, str(x)]) for x in range(n_cols)]\n",
    "\n",
    "    # Create the new dataframe\n",
    "    new_df = pd.DataFrame(new_dat.toarray(), \n",
    "                          index=data_ohc.index, \n",
    "                          columns=col_names)\n",
    "\n",
    "    # Append the new data to the dataframe\n",
    "    data_ohc = pd.concat([data_ohc, new_df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "215"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Column difference is as calculated above\n",
    "data_ohc.shape[1] - data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(data.shape[1])\n",
    "\n",
    "# Remove the string columns from the dataframe\n",
    "data = data.drop(num_ohc_cols.index, axis=1)\n",
    "\n",
    "print(data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 4\n",
    "\n",
    "* Create train and test splits of both data sets. To ensure the data gets split the same way, use the same `random_state` in each of the two splits.\n",
    "* For each data set, fit a basic linear regression model on the training data. \n",
    "* Calculate the mean squared error on both the train and test sets for the respective models. Which model produces smaller error on the test data and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "      1stFlrSF  2ndFlrSF  3SsnPorch  BedroomAbvGr  BsmtFinSF1  BsmtFinSF2  \\\n599     1518.0       0.0        0.0             1      1218.0         0.0   \n881      925.0       0.0        0.0             2       338.0       466.0   \n634     1095.0     679.0        0.0             4         0.0         0.0   \n425      888.0     868.0        0.0             3       742.0         0.0   \n906     1337.0       0.0        0.0             3       699.0         0.0   \n...        ...       ...        ...           ...         ...         ...   \n942     1077.0       0.0        0.0             3         0.0         0.0   \n59       581.0     530.0        0.0             3         0.0         0.0   \n405     1412.0       0.0        0.0             3      1005.0         0.0   \n458     1073.0       0.0        0.0             3       836.0         0.0   \n1300     984.0     620.0        0.0             3       408.0       420.0   \n\n      BsmtFullBath  BsmtHalfBath  BsmtUnfSF  EnclosedPorch  ...  OverallCond  \\\n599              0             0      300.0            0.0  ...            5   \n881              0             1      121.0            0.0  ...            6   \n634              1             0     1095.0           90.0  ...            2   \n425              1             0      130.0            0.0  ...            5   \n906              1             0      638.0            0.0  ...            5   \n...            ...           ...        ...            ...  ...          ...   \n942              0             0     1007.0           48.0  ...            4   \n59               0             0      530.0          144.0  ...            7   \n405              1             0      387.0          169.0  ...            5   \n458              1             0      237.0            0.0  ...            7   \n1300             0             0      156.0            0.0  ...            5   \n\n      OverallQual  PoolArea  ScreenPorch  TotRmsAbvGrd  TotalBsmtSF  \\\n599             8       0.0          0.0             6       1518.0   \n881             5       0.0          0.0             5        925.0   \n634             4       0.0          0.0             8       1095.0   \n425             6       0.0          0.0             7        872.0   \n906             7       0.0          0.0             6       1337.0   \n...           ...       ...          ...           ...          ...   \n942             3       0.0          0.0             6       1007.0   \n59              5       0.0          0.0             6        530.0   \n405             6       0.0          0.0             6       1392.0   \n458             5       0.0          0.0             6       1073.0   \n1300            5       0.0          0.0             6        984.0   \n\n      WoodDeckSF  YearBuilt  YearRemodAdd  YrSold  \n599        185.0       2003          2004    2008  \n881          0.0       1965          1965    2009  \n634          0.0       1900          1950    2006  \n425        144.0       1996          1997    2007  \n906          0.0       2003          2003    2007  \n...          ...        ...           ...     ...  \n942          0.0       1922          1950    2006  \n59           0.0       1920          1996    2007  \n405          0.0       1988          1988    2009  \n458          0.0       1965          1965    2007  \n1300         0.0       1941          1960    2009  \n\n[414 rows x 36 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1stFlrSF</th>\n      <th>2ndFlrSF</th>\n      <th>3SsnPorch</th>\n      <th>BedroomAbvGr</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>BsmtFullBath</th>\n      <th>BsmtHalfBath</th>\n      <th>BsmtUnfSF</th>\n      <th>EnclosedPorch</th>\n      <th>...</th>\n      <th>OverallCond</th>\n      <th>OverallQual</th>\n      <th>PoolArea</th>\n      <th>ScreenPorch</th>\n      <th>TotRmsAbvGrd</th>\n      <th>TotalBsmtSF</th>\n      <th>WoodDeckSF</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>599</th>\n      <td>1518.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>1218.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>300.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>8</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1518.0</td>\n      <td>185.0</td>\n      <td>2003</td>\n      <td>2004</td>\n      <td>2008</td>\n    </tr>\n    <tr>\n      <th>881</th>\n      <td>925.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>338.0</td>\n      <td>466.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>121.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>6</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5</td>\n      <td>925.0</td>\n      <td>0.0</td>\n      <td>1965</td>\n      <td>1965</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>634</th>\n      <td>1095.0</td>\n      <td>679.0</td>\n      <td>0.0</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1095.0</td>\n      <td>90.0</td>\n      <td>...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>8</td>\n      <td>1095.0</td>\n      <td>0.0</td>\n      <td>1900</td>\n      <td>1950</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>888.0</td>\n      <td>868.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>742.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>130.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7</td>\n      <td>872.0</td>\n      <td>144.0</td>\n      <td>1996</td>\n      <td>1997</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>906</th>\n      <td>1337.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>699.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>638.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1337.0</td>\n      <td>0.0</td>\n      <td>2003</td>\n      <td>2003</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>942</th>\n      <td>1077.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1007.0</td>\n      <td>48.0</td>\n      <td>...</td>\n      <td>4</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1007.0</td>\n      <td>0.0</td>\n      <td>1922</td>\n      <td>1950</td>\n      <td>2006</td>\n    </tr>\n    <tr>\n      <th>59</th>\n      <td>581.0</td>\n      <td>530.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>530.0</td>\n      <td>144.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>530.0</td>\n      <td>0.0</td>\n      <td>1920</td>\n      <td>1996</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>1412.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>1005.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>387.0</td>\n      <td>169.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1392.0</td>\n      <td>0.0</td>\n      <td>1988</td>\n      <td>1988</td>\n      <td>2009</td>\n    </tr>\n    <tr>\n      <th>458</th>\n      <td>1073.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>836.0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>237.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>7</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>1073.0</td>\n      <td>0.0</td>\n      <td>1965</td>\n      <td>1965</td>\n      <td>2007</td>\n    </tr>\n    <tr>\n      <th>1300</th>\n      <td>984.0</td>\n      <td>620.0</td>\n      <td>0.0</td>\n      <td>3</td>\n      <td>408.0</td>\n      <td>420.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>156.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6</td>\n      <td>984.0</td>\n      <td>0.0</td>\n      <td>1941</td>\n      <td>1960</td>\n      <td>2009</td>\n    </tr>\n  </tbody>\n</table>\n<p>414 rows × 36 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data_X, test_data_X, train_data_Y, test_data_Y = train_test_split(data.iloc[:, 0:-1], data.iloc[:, -1], random_state=42, train_size=.7)\n",
    "train_data_ohc_X, test_data_ohc_X, train_data_ohc_Y, test_data_ohc_Y = train_test_split(data_ohc.iloc[:, 0:-1], data_ohc.iloc[:, -1], random_state=42, train_size=.7)\n",
    "\n",
    "test_data_X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 360x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test = data.copy(deep=True)\n",
    "data_test[\"MasVnrArea\"].hist(figsize=(5, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "<AxesSubplot:>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_test[\"MasVnrArea_log\"] = np.log1p(data_test[\"MasVnrArea\"])\n",
    "data_test[\"MasVnrArea_log\"].hist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X=train_data_X)\n",
    "train_data_X_s = scaler.fit_transform(X=train_data_X)\n",
    "test_data_X_s = scaler.transform(X=test_data_X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "0.7727012498678243"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "lin = LinearRegression()\n",
    "lin.fit(X=train_data_X_s, y=train_data_Y)\n",
    "y_predict = lin.predict(X=test_data_X_s)\n",
    "\n",
    "r2_score(test_data_Y, y_predict)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "-0.002421307506053294"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_ohc_X_s = scaler.fit_transform(X=train_data_ohc_X)\n",
    "test_data_ohc_X_s = scaler.transform(X=test_data_ohc_X)\n",
    "lin.fit(X=train_data_ohc_X_s, y=train_data_ohc_Y)\n",
    "y_predict_ohs = lin.predict(X=test_data_ohc_X_s)\n",
    "\n",
    "r2_score(test_data_ohc_Y, y_predict_ohs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_col = 'SalePrice'\n",
    "\n",
    "# Split the data that is not one-hot encoded\n",
    "feature_cols = [x for x in data.columns if x != y_col]\n",
    "X_data = data[feature_cols]\n",
    "y_data = data[y_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n",
    "                                                    test_size=0.3, random_state=42)\n",
    "# Split the data that is one-hot encoded\n",
    "feature_cols = [x for x in data_ohc.columns if x != y_col]\n",
    "X_data_ohc = data_ohc[feature_cols]\n",
    "y_data_ohc = data_ohc[y_col]\n",
    "\n",
    "X_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc, \n",
    "                                                    test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare the indices to ensure they are identical\n",
    "(X_train_ohc.index == X_train.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "             no enc   one-hot enc\ntrain  1.131507e+09  3.177271e+08\ntest   1.372182e+09  1.972617e+16",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>no enc</th>\n      <th>one-hot enc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>train</th>\n      <td>1.131507e+09</td>\n      <td>3.177271e+08</td>\n    </tr>\n    <tr>\n      <th>test</th>\n      <td>1.372182e+09</td>\n      <td>1.972617e+16</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "LR = LinearRegression()\n",
    "\n",
    "# Storage for error values\n",
    "error_df = list()\n",
    "\n",
    "# Data that have not been one-hot encoded\n",
    "LR = LR.fit(X_train, y_train)\n",
    "y_train_pred = LR.predict(X_train)\n",
    "y_test_pred = LR.predict(X_test)\n",
    "\n",
    "error_df.append(pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n",
    "                           'test' : mean_squared_error(y_test,  y_test_pred)},\n",
    "                           name='no enc'))\n",
    "\n",
    "# Data that have been one-hot encoded\n",
    "LR = LR.fit(X_train_ohc, y_train_ohc)\n",
    "y_train_ohc_pred = LR.predict(X_train_ohc)\n",
    "y_test_ohc_pred = LR.predict(X_test_ohc)\n",
    "\n",
    "error_df.append(pd.Series({'train': mean_squared_error(y_train_ohc, y_train_ohc_pred),\n",
    "                           'test' : mean_squared_error(y_test_ohc,  y_test_ohc_pred)},\n",
    "                          name='one-hot enc'))\n",
    "\n",
    "# Assemble the results\n",
    "error_df = pd.concat(error_df, axis=1)\n",
    "error_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "Note that the error values on the one-hot encoded data are very different for the train and test data. In particular, the errors on the test data are much higher. Based on the lecture, this is because the one-hot encoded model is overfitting the data. We will learn how to deal with issues like this in the next lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "For each of the data sets (one-hot encoded and not encoded):\n",
    "\n",
    "* Scale the all the non-hot encoded values using one of the following: `StandardScaler`, `MinMaxScaler`, `MaxAbsScaler`.\n",
    "* Compare the error calculated on the test sets\n",
    "\n",
    "Be sure to calculate the skew (to decide if a transformation should be done) and fit the scaler on *ONLY* the training data, but then apply it to both the train and test data identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mute the setting wtih a copy warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "\n",
    "\n",
    "scalers = {'standard': StandardScaler(),\n",
    "           'minmax': MinMaxScaler(),\n",
    "           'maxabs': MaxAbsScaler()}\n",
    "\n",
    "training_test_sets = {\n",
    "    'not_encoded': (X_train, y_train, X_test, y_test),\n",
    "    'one_hot_encoded': (X_train_ohc, y_train_ohc, X_test_ohc, y_test_ohc)}\n",
    "\n",
    "\n",
    "# Get the list of float columns, and the float data\n",
    "# so that we don't scale something we already scaled. \n",
    "# We're supposed to scale the original data each time\n",
    "mask = X_train.dtypes == np.float\n",
    "float_columns = X_train.columns[mask]\n",
    "\n",
    "# initialize model\n",
    "LR = LinearRegression()\n",
    "\n",
    "# iterate over all possible combinations and get the errors\n",
    "errors = {}\n",
    "for encoding_label, (_X_train, _y_train, _X_test, _y_test) in training_test_sets.items():\n",
    "    for scaler_label, scaler in scalers.items():\n",
    "        trainingset = _X_train.copy()  # copy because we dont want to scale this more than once.\n",
    "        testset = _X_test.copy()\n",
    "        trainingset[float_columns] = scaler.fit_transform(trainingset[float_columns])\n",
    "        testset[float_columns] = scaler.transform(testset[float_columns])\n",
    "        LR.fit(trainingset, _y_train)\n",
    "        predictions = LR.predict(testset)\n",
    "        key = encoding_label + ' - ' + scaler_label + 'scaling'\n",
    "        errors[key] = mean_squared_error(_y_test, predictions)\n",
    "\n",
    "errors = pd.Series(errors)\n",
    "print(errors.to_string())\n",
    "print('-' * 80)\n",
    "for key, error_val in errors.items():\n",
    "    print(key, error_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "## Question 6\n",
    "\n",
    "Plot predictions vs actual for one of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "ax = plt.axes()\n",
    "# we are going to use y_test, y_test_pred\n",
    "ax.scatter(y_test, y_test_pred, alpha=.5)\n",
    "\n",
    "ax.set(xlabel='Ground truth', \n",
    "       ylabel='Predictions',\n",
    "       title='Ames, Iowa House Price Predictions vs Truth, using Linear Regression');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Machine Learning Foundation (C) 2020 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "name": "Train_Test_Splits_Regularization_Exercises-ANSWERS",
  "notebookId": 2125319687183944
 },
 "nbformat": 4,
 "nbformat_minor": 4
}